<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fundamental Research on Michael Mommert</title>
    <link>http://localhost:43263/categories/fundamental-research/</link>
    <description>Recent content in Fundamental Research on Michael Mommert</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Tue, 07 Jun 2022 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:43263/categories/fundamental-research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Contrastive Self-Supervised Learning for Multi-modal Earth Observation Data</title>
      <link>http://localhost:43263/research/2022-06-07-contrastive_self-supervised_data_fusion_for_satellite_imagery/</link>
      <pubDate>Tue, 07 Jun 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:43263/research/2022-06-07-contrastive_self-supervised_data_fusion_for_satellite_imagery/</guid>
      <description>&lt;p&gt;This research consists of two parts that will be presented in the following.&lt;/p&gt;&#xA;&lt;h1 id=&#34;contrastive-self-supervised-data-fusion-for-satellite-imagery&#34;&gt;Contrastive Self-supervised Data Fusion for Satellite Imagery&lt;/h1&gt;&#xA;&lt;p&gt;Supervised learning of any task requires large amounts of labeled data. Especially in the case of satellite imagery, unlabeled data is ubiquitous, while the labeling process is often cumbersome and expensive. Therefore, it is highly worthwhile to leverage methods to minimize the amount of labeled data that is required to obtain a good performance of the given down-stream task. In our first work, we leverage contrastive learning in a multi-modal setup to achieve this result.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
